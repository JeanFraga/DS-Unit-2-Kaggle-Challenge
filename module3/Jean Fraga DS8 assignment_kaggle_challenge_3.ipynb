{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lambda School Data Science, Unit 2: Predictive Modeling\n",
    "\n",
    "# Kaggle Challenge, Module 3\n",
    "\n",
    "\n",
    "## Assignment\n",
    "- [ ] [Review requirements for your portfolio project](https://lambdaschool.github.io/ds/unit2), then submit your dataset.\n",
    "- [ ] Continue to participate in our Kaggle challenge. \n",
    "- [ ] Use scikit-learn for hyperparameter optimization with RandomizedSearchCV.\n",
    "- [ ] Submit your predictions to our Kaggle competition. (Go to our Kaggle InClass competition webpage. Use the blue **Submit Predictions** button to upload your CSV file. Or you can use the Kaggle API to submit your predictions.)\n",
    "- [ ] Commit your notebook to your fork of the GitHub repo.\n",
    "\n",
    "## Stretch Goals\n",
    "\n",
    "### Reading\n",
    "- Jake VanderPlas, [Python Data Science Handbook, Chapter 5.3](https://jakevdp.github.io/PythonDataScienceHandbook/05.03-hyperparameters-and-model-validation.html), Hyperparameters and Model Validation\n",
    "- Jake VanderPlas, [Statistics for Hackers](https://speakerdeck.com/jakevdp/statistics-for-hackers?slide=107)\n",
    "- Ron Zacharski, [A Programmer's Guide to Data Mining, Chapter 5](http://guidetodatamining.com/chapter5/), 10-fold cross validation\n",
    "- Sebastian Raschka, [A Basic Pipeline and Grid Search Setup](https://github.com/rasbt/python-machine-learning-book/blob/master/code/bonus/svm_iris_pipeline_and_gridsearch.ipynb)\n",
    "- Peter Worcester, [A Comparison of Grid Search and Randomized Search Using Scikit Learn](https://blog.usejournal.com/a-comparison-of-grid-search-and-randomized-search-using-scikit-learn-29823179bc85)\n",
    "\n",
    "### Doing\n",
    "- Add your own stretch goals!\n",
    "- Try other [categorical encodings](https://contrib.scikit-learn.org/categorical-encoding/). See the previous assignment notebook for details.\n",
    "- In additon to `RandomizedSearchCV`, scikit-learn has [`GridSearchCV`](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). Another library called scikit-optimize has [`BayesSearchCV`](https://scikit-optimize.github.io/notebooks/sklearn-gridsearchcv-replacement.html). Experiment with these alternatives.\n",
    "- _[Introduction to Machine Learning with Python](http://shop.oreilly.com/product/0636920030515.do)_ discusses options for \"Grid-Searching Which Model To Use\" in Chapter 6:\n",
    "\n",
    "> You can even go further in combining GridSearchCV and Pipeline: it is also possible to search over the actual steps being performed in the pipeline (say whether to use StandardScaler or MinMaxScaler). This leads to an even bigger search space and should be considered carefully. Trying all possible solutions is usually not a viable machine learning strategy. However, here is an example comparing a RandomForestClassifier and an SVC ...\n",
    "\n",
    "The example is shown in [the accompanying notebook](https://github.com/amueller/introduction_to_ml_with_python/blob/master/06-algorithm-chains-and-pipelines.ipynb), code cells 35-37. Could you apply this concept to your own pipelines?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### BONUS: Stacking!\n",
    "\n",
    "Here's some code you can use to \"stack\" multiple submissions, which is another form of ensembling:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Filenames of your submissions you want to ensemble\n",
    "files = ['submission-01.csv', 'submission-02.csv', 'submission-03.csv']\n",
    "\n",
    "target = 'status_group'\n",
    "submissions = (pd.read_csv(file)[[target]] for file in files)\n",
    "ensemble = pd.concat(submissions, axis='columns')\n",
    "majority_vote = ensemble.mode(axis='columns')[0]\n",
    "\n",
    "sample_submission = pd.read_csv('sample_submission.csv')\n",
    "submission = sample_submission.copy()\n",
    "submission[target] = majority_vote\n",
    "submission.to_csv('my-ultimate-ensemble-submission.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ignore this Numpy warning when using Plotly Express:\n",
    "# FutureWarning: Method .ptp is deprecated and will be removed in a future version. Use numpy.ptp instead.\n",
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning, module='numpy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import sys\n",
    "\n",
    "# If you're on Colab:\n",
    "if 'google.colab' in sys.modules:\n",
    "    DATA_PATH = 'https://raw.githubusercontent.com/LambdaSchool/DS-Unit-2-Kaggle-Challenge/master/data/'\n",
    "    !pip install category_encoders==2.*\n",
    "\n",
    "# If you're working locally:\n",
    "else:\n",
    "    DATA_PATH = '../data/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge train_features.csv & train_labels.csv\n",
    "train = pd.merge(pd.read_csv(DATA_PATH+'waterpumps/train_features.csv'), \n",
    "                 pd.read_csv(DATA_PATH+'waterpumps/train_labels.csv'))\n",
    "\n",
    "# Read test_features.csv & sample_submission.csv\n",
    "test = pd.read_csv(DATA_PATH+'waterpumps/test_features.csv')\n",
    "sample_submission = pd.read_csv(DATA_PATH+'waterpumps/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((59400, 41), (14358, 40), (59400, 41))"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.merge(pd.read_csv(DATA_PATH+'waterpumps/train_features.csv'), \n",
    "                 pd.read_csv(DATA_PATH+'waterpumps/train_labels.csv'))\n",
    "test = pd.read_csv(DATA_PATH+'waterpumps/test_features.csv')\n",
    "sample_submission = pd.read_csv(DATA_PATH+'waterpumps/sample_submission.csv')\n",
    "final = train.copy()\n",
    "\n",
    "train.shape, test.shape, final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import category_encoders as ce\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "import pandas_profiling\n",
    "from sklearn.feature_selection import f_regression, SelectKBest\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from scipy.stats import randint, uniform\n",
    "from sklearn.pipeline import make_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((47520, 41), (11880, 41))"
      ]
     },
     "execution_count": 181,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train, val = train_test_split(train, train_size=0.80, test_size=0.20, \n",
    "                              stratify=train['status_group'], random_state=7)\n",
    "train.shape, val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wrangle(X):\n",
    "    \"\"\"Wrangle train, validate, and test sets in the same way\"\"\"\n",
    "    \n",
    "    # Prevent SettingWithCopyWarning\n",
    "    X = X.copy()\n",
    "    \n",
    "    # About 3% of the time, latitude has small values near zero,\n",
    "    # outside Tanzania, so we'll treat these values like zero.\n",
    "    X['latitude'] = X['latitude'].replace(-2e-08, 0)\n",
    "    X['gps_height'] = X['gps_height'].replace([-90], X['gps_height'].mean())\n",
    "    # When columns have zeros and shouldn't, they are like null values.\n",
    "    # So we will replace the zeros with nulls, and impute missing values later.\n",
    "    cols_with_zeros = ['longitude', 'latitude', 'construction_year', 'population']\n",
    "    for col in cols_with_zeros:\n",
    "        X[col] = X[col].replace(0, np.nan)\n",
    "    #X.date_recorded = X['date_recorded'].str[:4]#.astype('float64')\n",
    "    #X.construction_year = X['construction_year'].astype('str').str[:4].astype('float64')\n",
    "    # quantity & quantity_group are duplicates, num_private has no discernible use.\n",
    "    X = X.drop(columns=['quantity_group','num_private', 'recorded_by', 'payment_type', 'id'])\n",
    "    X['date_recorded'] = pd.to_datetime(X['date_recorded'], infer_datetime_format=True)\n",
    "    \n",
    "    # Extract components from date_recorded, then drop the original column\n",
    "    X['year_recorded'] = X['date_recorded'].dt.year\n",
    "    X['month_recorded'] = X['date_recorded'].dt.month\n",
    "    X['day_recorded'] = X['date_recorded'].dt.day\n",
    "    X = X.drop(columns='date_recorded')\n",
    "    X['years'] = X['year_recorded'] - X['construction_year']\n",
    "    X['years_MISSING'] = X['years'].isnull()\n",
    "    # return the wrangled dataframe\n",
    "    return X\n",
    "\n",
    "final = wrangle(final)\n",
    "train = wrangle(train)\n",
    "val = wrangle(val)\n",
    "test = wrangle(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59400"
      ]
     },
     "execution_count": 183,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['amount_tsh', 'gps_height', 'longitude', 'latitude', 'region_code', 'district_code', 'population', 'construction_year', 'year_recorded', 'month_recorded', 'day_recorded', 'years', 'basin', 'region', 'lga', 'public_meeting', 'scheme_management', 'permit', 'extraction_type', 'extraction_type_group', 'extraction_type_class', 'management', 'management_group', 'payment', 'water_quality', 'quality_group', 'quantity', 'source', 'source_type', 'source_class', 'waterpoint_type', 'waterpoint_type_group', 'years_MISSING', 'funder', 'installer', 'wpt_name', 'subvillage', 'ward', 'scheme_name']\n"
     ]
    }
   ],
   "source": [
    "# The status_group column is the target\n",
    "target = 'status_group'\n",
    "\n",
    "# Get a dataframe with all train columns except the target & id\n",
    "train_features = train.drop(columns=[target])\n",
    "\n",
    "# Get a series with the cardinality of the nonnumeric features\n",
    "cardinality = train_features.select_dtypes(exclude='number').nunique()\n",
    "\n",
    "# Get a list of the numeric features\n",
    "numeric_features = train_features.select_dtypes(include='number').columns.tolist()\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('scaler', StandardScaler())])\n",
    "\n",
    "# Get a list of all categorical features with cardinality <= 50\n",
    "\n",
    "categorical_features = cardinality[cardinality <= 130].index.tolist()\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', ce.OneHotEncoder(use_cat_names=True)),\n",
    "    ('kbest', SelectKBest(k = 100))])\n",
    "\n",
    "ordinal_features = cardinality[cardinality >= 130].index.tolist()\n",
    "ordinal_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('ordinal', ce.OrdinalEncoder()),\n",
    "    ('kbest', SelectKBest(k = 'all'))])\n",
    "\n",
    "target_features = target\n",
    "target_transformer = Pipeline(steps=[('label', LabelEncoder())])\n",
    "\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('ord', ordinal_transformer, ordinal_features)])\n",
    "\n",
    "# ('tar', target_transformer, target_features)\n",
    "\n",
    "features = numeric_features + categorical_features + ordinal_features\n",
    "\n",
    "X_train = train[features]\n",
    "y_train = train[target]\n",
    "X_val = val[features]\n",
    "y_val = val[target]\n",
    "X_test = test[features]\n",
    "X_final = final[features]\n",
    "y_final = final[target]\n",
    "\n",
    "\n",
    "#X_final = X_final[(X_final[numeric_features] >= np.percentile(X_final[numeric_features], 0.5))]\n",
    "# X_train = train.drop(columns=target)\n",
    "# y_train = train.drop(columns=features)\n",
    "# X_val = val.drop(columns=target)\n",
    "# y_val = val[target]\n",
    "# X_test = test\n",
    "# X_final = final.drop(columns=target)\n",
    "# y_final = final[target]\n",
    "\n",
    "# Combine the lists \n",
    "# features = numeric_features + categorical_features + ordinal_features\n",
    "print(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['amount_tsh',\n",
       " 'gps_height',\n",
       " 'longitude',\n",
       " 'latitude',\n",
       " 'region_code',\n",
       " 'district_code',\n",
       " 'population',\n",
       " 'construction_year',\n",
       " 'year_recorded',\n",
       " 'month_recorded',\n",
       " 'day_recorded',\n",
       " 'years']"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numeric_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26660        functional\n",
       "46901        functional\n",
       "1058         functional\n",
       "9559     non functional\n",
       "35556        functional\n",
       "Name: status_group, dtype: object"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>amount_tsh</th>\n",
       "      <th>gps_height</th>\n",
       "      <th>longitude</th>\n",
       "      <th>latitude</th>\n",
       "      <th>region_code</th>\n",
       "      <th>district_code</th>\n",
       "      <th>population</th>\n",
       "      <th>construction_year</th>\n",
       "      <th>year_recorded</th>\n",
       "      <th>month_recorded</th>\n",
       "      <th>...</th>\n",
       "      <th>source_class</th>\n",
       "      <th>waterpoint_type</th>\n",
       "      <th>waterpoint_type_group</th>\n",
       "      <th>years_MISSING</th>\n",
       "      <th>funder</th>\n",
       "      <th>installer</th>\n",
       "      <th>wpt_name</th>\n",
       "      <th>subvillage</th>\n",
       "      <th>ward</th>\n",
       "      <th>scheme_name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>26660</th>\n",
       "      <td>0.0</td>\n",
       "      <td>1138.0</td>\n",
       "      <td>36.901195</td>\n",
       "      <td>-3.391070</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>350.0</td>\n",
       "      <td>2009.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>surface</td>\n",
       "      <td>communal standpipe</td>\n",
       "      <td>communal standpipe</td>\n",
       "      <td>False</td>\n",
       "      <td>Rotary Club</td>\n",
       "      <td>Rotary Club</td>\n",
       "      <td>Kitefu Primary School</td>\n",
       "      <td>Tanesco</td>\n",
       "      <td>Maji ya Chai</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46901</th>\n",
       "      <td>3000.0</td>\n",
       "      <td>910.0</td>\n",
       "      <td>35.366883</td>\n",
       "      <td>-7.642936</td>\n",
       "      <td>11</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1992.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>groundwater</td>\n",
       "      <td>communal standpipe</td>\n",
       "      <td>communal standpipe</td>\n",
       "      <td>False</td>\n",
       "      <td>Danida</td>\n",
       "      <td>DANID</td>\n",
       "      <td>none</td>\n",
       "      <td>Ndorobo B</td>\n",
       "      <td>Mlowa</td>\n",
       "      <td>Mlowa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1058</th>\n",
       "      <td>1500.0</td>\n",
       "      <td>682.0</td>\n",
       "      <td>37.512633</td>\n",
       "      <td>-3.542461</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>250.0</td>\n",
       "      <td>2011.0</td>\n",
       "      <td>2013</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>groundwater</td>\n",
       "      <td>communal standpipe</td>\n",
       "      <td>communal standpipe</td>\n",
       "      <td>False</td>\n",
       "      <td>Government Of Tanzania</td>\n",
       "      <td>District council</td>\n",
       "      <td>Kwa Salim Idd</td>\n",
       "      <td>Kitopeni</td>\n",
       "      <td>Kileo</td>\n",
       "      <td>Kifaru water Supply</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9559</th>\n",
       "      <td>0.0</td>\n",
       "      <td>370.0</td>\n",
       "      <td>38.688599</td>\n",
       "      <td>-4.847517</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>150.0</td>\n",
       "      <td>1975.0</td>\n",
       "      <td>2011</td>\n",
       "      <td>3</td>\n",
       "      <td>...</td>\n",
       "      <td>groundwater</td>\n",
       "      <td>communal standpipe</td>\n",
       "      <td>communal standpipe</td>\n",
       "      <td>False</td>\n",
       "      <td>Government Of Tanzania</td>\n",
       "      <td>RWE</td>\n",
       "      <td>Msikitini</td>\n",
       "      <td>Dukani</td>\n",
       "      <td>Kizara</td>\n",
       "      <td>Bombomajimoto water</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35556</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>34.156026</td>\n",
       "      <td>-3.104921</td>\n",
       "      <td>17</td>\n",
       "      <td>6</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2012</td>\n",
       "      <td>10</td>\n",
       "      <td>...</td>\n",
       "      <td>groundwater</td>\n",
       "      <td>hand pump</td>\n",
       "      <td>hand pump</td>\n",
       "      <td>True</td>\n",
       "      <td>Rwssp</td>\n",
       "      <td>WEDECO</td>\n",
       "      <td>Polewalandi</td>\n",
       "      <td>Makungu</td>\n",
       "      <td>Kisesa</td>\n",
       "      <td>None</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       amount_tsh  gps_height  longitude  latitude  region_code  \\\n",
       "26660         0.0      1138.0  36.901195 -3.391070            2   \n",
       "46901      3000.0       910.0  35.366883 -7.642936           11   \n",
       "1058       1500.0       682.0  37.512633 -3.542461            3   \n",
       "9559          0.0       370.0  38.688599 -4.847517            4   \n",
       "35556         0.0         0.0  34.156026 -3.104921           17   \n",
       "\n",
       "       district_code  population  construction_year  year_recorded  \\\n",
       "26660              7       350.0             2009.0           2013   \n",
       "46901              1         1.0             1992.0           2011   \n",
       "1058               2       250.0             2011.0           2013   \n",
       "9559               2       150.0             1975.0           2011   \n",
       "35556              6         NaN                NaN           2012   \n",
       "\n",
       "       month_recorded  ...  source_class     waterpoint_type  \\\n",
       "26660               2  ...       surface  communal standpipe   \n",
       "46901               3  ...   groundwater  communal standpipe   \n",
       "1058                2  ...   groundwater  communal standpipe   \n",
       "9559                3  ...   groundwater  communal standpipe   \n",
       "35556              10  ...   groundwater           hand pump   \n",
       "\n",
       "      waterpoint_type_group years_MISSING                  funder  \\\n",
       "26660    communal standpipe         False             Rotary Club   \n",
       "46901    communal standpipe         False                  Danida   \n",
       "1058     communal standpipe         False  Government Of Tanzania   \n",
       "9559     communal standpipe         False  Government Of Tanzania   \n",
       "35556             hand pump          True                   Rwssp   \n",
       "\n",
       "              installer               wpt_name subvillage          ward  \\\n",
       "26660       Rotary Club  Kitefu Primary School    Tanesco  Maji ya Chai   \n",
       "46901             DANID                   none  Ndorobo B         Mlowa   \n",
       "1058   District council          Kwa Salim Idd   Kitopeni         Kileo   \n",
       "9559                RWE              Msikitini     Dukani        Kizara   \n",
       "35556            WEDECO            Polewalandi    Makungu        Kisesa   \n",
       "\n",
       "               scheme_name  \n",
       "26660                  NaN  \n",
       "46901                Mlowa  \n",
       "1058   Kifaru water Supply  \n",
       "9559   Bombomajimoto water  \n",
       "35556                 None  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:947: ConvergenceWarning: lbfgs failed to converge. Increase the number of iterations.\n",
      "  \"of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "         steps=[('preprocessor',\n",
       "                 ColumnTransformer(n_jobs=None, remainder='drop',\n",
       "                                   sparse_threshold=0.3,\n",
       "                                   transformer_weights=None,\n",
       "                                   transformers=[('num',\n",
       "                                                  Pipeline(memory=None,\n",
       "                                                           steps=[('imputer',\n",
       "                                                                   SimpleImputer(add_indicator=False,\n",
       "                                                                                 copy=True,\n",
       "                                                                                 fill_value=None,\n",
       "                                                                                 missing_values=nan,\n",
       "                                                                                 strategy='most_frequent',\n",
       "                                                                                 verbose=0)),\n",
       "                                                                  ('scaler',\n",
       "                                                                   StandardScaler(copy=True,\n",
       "                                                                                  wi...\n",
       "                                                  ['funder', 'installer',\n",
       "                                                   'wpt_name', 'subvillage',\n",
       "                                                   'ward', 'scheme_name'])],\n",
       "                                   verbose=False)),\n",
       "                ('classifier',\n",
       "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
       "                                    fit_intercept=True, intercept_scaling=1,\n",
       "                                    l1_ratio=None, max_iter=100,\n",
       "                                    multi_class='warn', n_jobs=None,\n",
       "                                    penalty='l2', random_state=None,\n",
       "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
       "                                    warm_start=False))],\n",
       "         verbose=False)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "#                       ('classifier', RandomForestRegressor(random_state=7))])\n",
    "\n",
    "# pipeline = make_pipeline(\n",
    "#     preprocessor, \n",
    "#     RandomForestRegressor(random_state=7)\n",
    "# )\n",
    "\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', LogisticRegression(solver='lbfgs'))])\n",
    "\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Score on val\n",
    "#print('Validation Accuracy', pipeline.score(X_val, y_val))\n",
    "\n",
    "# Predict on test\n",
    "#y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy 0.6749158249158249\n"
     ]
    }
   ],
   "source": [
    "#Score on val\n",
    "print('Validation Accuracy', pipeline.score(X_val, y_val))\n",
    "\n",
    "#Predict on test\n",
    "y_pred = pipeline.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Accuracy 0.9999789562289563\n",
      "Validation Accuracy 0.8168350168350168\n"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                      ('classifier', RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1))])\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "print('Train Accuracy', pipeline.score(X_train, y_train))\n",
    "print('Validation Accuracy', pipeline.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My baseline here is the same as the submission I got in kaggle. This accuracy came out to be 0.81501."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Accuracy 0.999983164983165\n",
      "Successfully submitted to DS8 Predictive Modeling Challenge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/279k [00:00<?, ?B/s]\n",
      " 40%|####      | 112k/279k [00:00<00:00, 1.14MB/s]\n",
      " 78%|#######7  | 216k/279k [00:00<00:00, 1.08MB/s]\n",
      "100%|##########| 279k/279k [00:01<00:00, 167kB/s] \n"
     ]
    }
   ],
   "source": [
    "pipeline.fit(X_final, y_final)\n",
    "print('Final Accuracy', pipeline.score(X_final, y_final))\n",
    "y_pred = pipeline.predict(X_test)\n",
    "sample_submission = pd.read_csv(DATA_PATH+'waterpumps/sample_submission.csv')\n",
    "submission = sample_submission.copy()\n",
    "submission['status_group'] = y_pred\n",
    "submission.to_csv('your-submission-filename.csv', index=False)\n",
    "!kaggle competitions submit -c ds8-predictive-modeling-challenge -f your-submission-filename.csv -m \"Using preprocessing to encode everything in a different manner\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "exceptions must derive from BaseException",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-100-8131babb8f75>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# ('preprocessor', preprocessor),\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Train Accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpipeline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mscore\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    350\u001b[0m             \u001b[0mThis\u001b[0m \u001b[0mestimator\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    351\u001b[0m         \"\"\"\n\u001b[1;32m--> 352\u001b[1;33m         \u001b[0mXt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfit_params\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    353\u001b[0m         with _print_elapsed_time('Pipeline',\n\u001b[0;32m    354\u001b[0m                                  self._log_message(len(self.steps) - 1)):\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    315\u001b[0m                 \u001b[0mmessage_clsname\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Pipeline'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m                 \u001b[0mmessage\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_log_message\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstep_idx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m                 **fit_params_steps[name])\n\u001b[0m\u001b[0;32m    318\u001b[0m             \u001b[1;31m# Replace the transformer of the step with the fitted\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m             \u001b[1;31m# transformer. This is necessary when loading the transformer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\joblib\\memory.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    353\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 355\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    356\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    357\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mcall_and_shelve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\u001b[0m in \u001b[0;36m_fit_transform_one\u001b[1;34m(transformer, X, y, weight, message_clsname, message, **fit_params)\u001b[0m\n\u001b[0;32m    714\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0m_print_elapsed_time\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmessage_clsname\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    715\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtransformer\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 716\u001b[1;33m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    717\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    718\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtransformer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\category_encoders\\target_encoder.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[0;32m    254\u001b[0m         \u001b[1;31m# the interface requires 'y=None' in the signature but we need 'y'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    255\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 256\u001b[1;33m             \u001b[1;32mraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'fit_transform() missing argument: '\u001b[0m\u001b[1;34m'y'\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    257\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    258\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: exceptions must derive from BaseException"
     ]
    }
   ],
   "source": [
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                           ('target_encoder', ce.TargetEncoder),\n",
    "                           ('classifier', RandomForestRegressor(random_state=7))])\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "print('Train Accuracy', pipeline.score(X_train, y_train))\n",
    "print('Validation Accuracy', pipeline.score(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 5 candidates, totalling 15 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done   2 out of  15 | elapsed:   51.5s remaining:  5.6min\n",
      "[Parallel(n_jobs=-1)]: Done   4 out of  15 | elapsed:   56.6s remaining:  2.6min\n",
      "[Parallel(n_jobs=-1)]: Done   6 out of  15 | elapsed:   59.8s remaining:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done   8 out of  15 | elapsed:  1.1min remaining:   55.6s\n",
      "[Parallel(n_jobs=-1)]: Done  10 out of  15 | elapsed:  1.7min remaining:   51.1s\n",
      "[Parallel(n_jobs=-1)]: Done  12 out of  15 | elapsed:  1.9min remaining:   28.5s\n",
      "[Parallel(n_jobs=-1)]: Done  15 out of  15 | elapsed:  2.1min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters {'classifier__max_depth': 70, 'classifier__max_features': 0.23786198843999357, 'classifier__n_estimators': 230, 'preprocessor__cat__kbest__k': 39, 'preprocessor__ord__kbest__k': 6}\n",
      "Cross-validation MAE -0.8037205387205387\n"
     ]
    }
   ],
   "source": [
    "# pipeline = make_pipeline(\n",
    "#     ce.TargetEncoder(), \n",
    "#     SimpleImputer(), \n",
    "#     RandomForestRegressor(random_state=42)\n",
    "# )\n",
    "from scipy.stats import randint, uniform\n",
    "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
    "                          # ('targetencoder',ce.TargetEncoder()),\n",
    "                      ('classifier', RandomForestClassifier(n_estimators=100, random_state=42,n_jobs=-1))])\n",
    "param_distributions = {\n",
    "#    'targetencoder__min_samples_leaf': randint(1, 1000), \n",
    "#    'targetencoder__smoothing': uniform(1, 1000), \n",
    "#    'simpleimputer__strategy': ['mean', 'median'], \n",
    "    #'preprocessor__num__imputer__strategy': ['mean', 'most_frequent'],\n",
    "    'preprocessor__cat__kbest__k': range(0, len(X_train.columns)+1),\n",
    "    'preprocessor__ord__kbest__k': range(5,7),\n",
    "    'classifier__n_estimators': randint(200, 300), \n",
    "    'classifier__max_depth': [60, 65, 70], \n",
    "    'classifier__max_features': uniform(0, .5), \n",
    "}\n",
    "\n",
    "# If you're on Colab, decrease n_iter & cv parameters\n",
    "search = RandomizedSearchCV(\n",
    "    pipeline, \n",
    "    param_distributions=param_distributions, \n",
    "    n_iter=5, \n",
    "    cv=3, \n",
    "    #scoring='neg_mean_absolute_error', \n",
    "    verbose=10, \n",
    "    return_train_score=True, \n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "search.fit(X_final, y_final);\n",
    "\n",
    "print('Best hyperparameters', search.best_params_)\n",
    "print('Cross-validation MAE', -search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully submitted to DS8 Predictive Modeling Challenge\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|          | 0.00/278k [00:00<?, ?B/s]\n",
      " 43%|####3     | 120k/278k [00:00<00:00, 1.17MB/s]\n",
      " 78%|#######7  | 216k/278k [00:00<00:00, 1.10MB/s]\n",
      "100%|##########| 278k/278k [00:01<00:00, 146kB/s] \n"
     ]
    }
   ],
   "source": [
    "pipeline = search.best_estimator_\n",
    "y_pred = pipeline.predict(X_test)\n",
    "sample_submission = pd.read_csv(DATA_PATH+'waterpumps/sample_submission.csv')\n",
    "submission = sample_submission.copy()\n",
    "submission['status_group'] = y_pred\n",
    "submission.to_csv('your-submission-filename.csv', index=False)\n",
    "!kaggle competitions submit -c ds8-predictive-modeling-challenge -f your-submission-filename.csv -m \"Using randomized searchCV got a .79 MAE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# param_grid = {\n",
    "#     'preprocessor__num__imputer__strategy': ['mean', 'median'],\n",
    "#     'classifier__C': [0.1, 1.0, 10, 100],\n",
    "# }\n",
    "\n",
    "# grid_search = GridSearchCV(clf, param_grid, cv=10, iid=False)\n",
    "# grid_search.fit(X_train, y_train)\n",
    "\n",
    "# print((\"best logistic regression from grid search: %.3f\"\n",
    "#        % grid_search.score(X_test, y_test)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
